{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Homework 3: NumPy, Dask, and XGBoost\n",
        "\n",
        "This notebook consolidates Parts 1–3 of the assignment into a single workflow. Run the cells in order on the Explorer OOD Jupyter environment after ensuring the required packages are installed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment Notes\n",
        "- Recommended conda environment: `module load anaconda`, then create/activate one with NumPy, Dask, XGBoost, scikit-learn, python-docx, nycflights13.\n",
        "- If packages are missing run: `pip install numpy dask[complete] distributed xgboost scikit-learn python-docx nycflights13`.\n",
        "- Upload `train.csv` into the same directory as this notebook before starting Part 2.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import dask.array as da\n",
        "import dask.dataframe as dd\n",
        "from dask.distributed import Client, LocalCluster, performance_report\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    from xgboost import dask as dxgb\n",
        "    from xgboost import XGBClassifier\n",
        "except ImportError as exc:  # pragma: no cover - import guard\n",
        "    raise ImportError(\"Install xgboost with `pip install xgboost` before running Part 2.\") from exc\n",
        "\n",
        "try:\n",
        "    from docx import Document\n",
        "except ImportError:  # pragma: no cover - import guard\n",
        "    Document = None\n",
        "    print(\"python-docx is not installed; install it with `pip install python-docx` before running the reporting cell.\")\n",
        "\n",
        "plt.style.use(\"seaborn-v0_8\")\n",
        "\n",
        "SEED = 1\n",
        "rng = np.random.default_rng(SEED)\n",
        "\n",
        "part1_results = {}\n",
        "part2_results = {\"cpu_train_times\": {}, \"cpu_accuracy\": {}, \"dask_train_time\": None, \"dask_accuracy\": None}\n",
        "part3_results = {}\n",
        "\n",
        "OUTPUT_DIR = Path.cwd()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1 – NumPy Array vs. Dask Array\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. NumPy Array Creation and Row Means\n",
        "Use a fixed seed (1) and generate a 30,000 × 20,000 integer array with values in `[1, 10)`. The array is stored as `int16` to keep memory demands manageable while satisfying the requirement. A simple timer measures the time to compute row means.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ARRAY_SHAPE = (30000, 20000)\n",
        "ARRAY_DTYPE = np.int16\n",
        "\n",
        "start = time.perf_counter()\n",
        "np_array = rng.integers(1, 10, size=ARRAY_SHAPE, dtype=ARRAY_DTYPE)\n",
        "row_means_np = np_array.mean(axis=1)\n",
        "elapsed = time.perf_counter() - start\n",
        "\n",
        "part1_results[\"numpy_row_mean_seconds\"] = elapsed\n",
        "\n",
        "print(f\"NumPy row means computed in {elapsed:.2f} seconds.\")\n",
        "print(f\"Array dtype: {np_array.dtype}, total bytes: {np_array.nbytes / 1e9:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Convert to Dask Array with `(1000, 1000)` Chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chunk_shape = (1000, 1000)\n",
        "da4 = da.from_array(np_array, chunks=chunk_shape, name=\"da4\")\n",
        "da4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Compare Byte Sizes for Array and Chunks\n",
        "Use NumPy to compute expected byte sizes and compare them to Dask’s own metadata (`da4.nbytes`, chunk sizing).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "array_bytes = np_array.nbytes\n",
        "chunk_bytes_calc = int(np.prod(chunk_shape) * np_array.dtype.itemsize)\n",
        "\n",
        "# Inspect Dask metadata\n",
        "reported_array_bytes = da4.nbytes\n",
        "\n",
        "# Derive one representative chunk shape from Dask's chunk structure\n",
        "chunk_shapes = (da4.chunks[0][0], da4.chunks[1][0])\n",
        "reported_chunk_bytes = int(np.prod(chunk_shapes) * np_array.dtype.itemsize)\n",
        "\n",
        "comparison = {\n",
        "    \"numpy_array_bytes\": array_bytes,\n",
        "    \"numpy_chunk_bytes\": chunk_bytes_calc,\n",
        "    \"dask_array_bytes\": reported_array_bytes,\n",
        "    \"dask_sample_chunk_bytes\": reported_chunk_bytes,\n",
        "}\n",
        "\n",
        "part1_results[\"byte_size_comparison\"] = comparison\n",
        "\n",
        "print(json.dumps(comparison, indent=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Chunk Experiments & Timing\n",
        "Time the Dask mean calculation for three chunk configurations, then visualize the runtime differences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chunk_experiments = {\n",
        "    \"(1000, 1000)\": (1000, 1000),\n",
        "    \"(3000, 2000)\": (3000, 2000),\n",
        "    \"(300, 200)\": (300, 200),\n",
        "}\n",
        "\n",
        "chunk_timings = {}\n",
        "for label, chunk in chunk_experiments.items():\n",
        "    darr = da.from_array(np_array, chunks=chunk)\n",
        "    start = time.perf_counter()\n",
        "    _ = darr.mean(axis=1).compute()\n",
        "    elapsed = time.perf_counter() - start\n",
        "    chunk_timings[label] = elapsed\n",
        "    print(f\"Chunk {label}: {elapsed:.2f} seconds\")\n",
        "\n",
        "part1_results[\"chunk_timings_seconds\"] = chunk_timings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "labels = list(chunk_timings.keys())\n",
        "values = [chunk_timings[label] for label in labels]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 4))\n",
        "ax.bar(labels, values, color=\"#4472c4\")\n",
        "ax.set_ylabel(\"Seconds\")\n",
        "ax.set_title(\"Dask mean runtime vs. chunk size\")\n",
        "for idx, val in enumerate(values):\n",
        "    ax.text(idx, val, f\"{val:.2f}s\", ha=\"center\", va=\"bottom\")\n",
        "plt.xticks(rotation=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The most efficient chunking strategy balances parallelism and task overhead. Larger chunks reduce scheduling overhead but may strain memory; smaller chunks increase parallelism but incur more scheduler coordination.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2 – XGBoost Classification Workflow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Load `train.csv`\n",
        "Ensure the dataset resides next to this notebook. If the file path differs, update `DATA_PATH` below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATA_PATH = Path(\"train.csv\")\n",
        "if not DATA_PATH.exists():\n",
        "    raise FileNotFoundError(f\"Expected to find {DATA_PATH.resolve()} – upload the dataset before proceeding.\")\n",
        "\n",
        "raw_df = pd.read_csv(DATA_PATH)\n",
        "print(f\"Loaded dataset with shape: {raw_df.shape}\")\n",
        "raw_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Preprocess Features and Target\n",
        "- Update `TARGET_COLUMN` if needed.\n",
        "- Encodes string labels to integers when necessary.\n",
        "- Splits data into training/testing sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TARGET_COLUMN = \"target\"  # Update if your dataset uses a different label column name.\n",
        "if TARGET_COLUMN not in raw_df.columns:\n",
        "    TARGET_COLUMN = raw_df.columns[-1]\n",
        "    print(f\"TARGET_COLUMN not found – defaulting to last column: {TARGET_COLUMN}\")\n",
        "\n",
        "feature_df = raw_df.drop(columns=[TARGET_COLUMN])\n",
        "target_series = raw_df[TARGET_COLUMN]\n",
        "\n",
        "label_encoder = None\n",
        "if target_series.dtype == object:\n",
        "    label_encoder = LabelEncoder()\n",
        "    target_encoded = label_encoder.fit_transform(target_series)\n",
        "else:\n",
        "    target_encoded = target_series.to_numpy()\n",
        "\n",
        "X = feature_df\n",
        "y = pd.Series(target_encoded, name=\"target_encoded\")\n",
        "\n",
        "X_train_df, X_test_df, y_train_series, y_test_series = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.2,\n",
        "    random_state=SEED,\n",
        "    stratify=y,\n",
        ")\n",
        "\n",
        "num_classes = int(y.nunique())\n",
        "objective = \"binary:logistic\" if num_classes == 2 else \"multi:softprob\"\n",
        "print(f\"Objective: {objective}; classes: {num_classes}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Train XGBoost with Varying CPU Counts\n",
        "Measure training time for `n_jobs = {1, 2, 4, 8}` and collect accuracy for comparison.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_xgboost_cpu(n_jobs: int):\n",
        "    params = dict(\n",
        "        n_estimators=200,\n",
        "        learning_rate=0.1,\n",
        "        max_depth=6,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        objective=objective,\n",
        "        tree_method=\"hist\",\n",
        "        eval_metric=\"logloss\",\n",
        "        n_jobs=n_jobs,\n",
        "        random_state=SEED,\n",
        "    )\n",
        "    if num_classes > 2:\n",
        "        params[\"num_class\"] = num_classes\n",
        "\n",
        "    model = XGBClassifier(**params)\n",
        "    start = time.perf_counter()\n",
        "    model.fit(X_train_df, y_train_series)\n",
        "    elapsed = time.perf_counter() - start\n",
        "\n",
        "    y_pred = model.predict(X_test_df)\n",
        "    acc = accuracy_score(y_test_series, y_pred)\n",
        "    return model, elapsed, acc\n",
        "\n",
        "cpu_options = [1, 2, 4, 8]\n",
        "xgb_models = {}\n",
        "\n",
        "for n_jobs in cpu_options:\n",
        "    model, elapsed, acc = train_xgboost_cpu(n_jobs)\n",
        "    xgb_models[n_jobs] = model\n",
        "    part2_results[\"cpu_train_times\"][n_jobs] = elapsed\n",
        "    part2_results[\"cpu_accuracy\"][n_jobs] = acc\n",
        "    print(f\"n_jobs={n_jobs}: {elapsed:.2f} seconds, accuracy={acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(6, 4))\n",
        "ax.bar(part2_results[\"cpu_train_times\"].keys(), part2_results[\"cpu_train_times\"].values(), color=\"#ed7d31\")\n",
        "ax.set_xlabel(\"n_jobs\")\n",
        "ax.set_ylabel(\"Seconds\")\n",
        "ax.set_title(\"XGBoost training time vs. CPU count\")\n",
        "for idx, (n_jobs, seconds) in enumerate(part2_results[\"cpu_train_times\"].items()):\n",
        "    ax.text(idx, seconds, f\"{seconds:.2f}s\", ha=\"center\", va=\"bottom\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Provision Dask Distributed Scheduler (4 Cores, `processes=False`)\n",
        "Create a local Dask cluster that mirrors the Explorer allocation of 4 cores with one thread per worker.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cluster = LocalCluster(\n",
        "    n_workers=4,\n",
        "    threads_per_worker=1,\n",
        "    processes=False,\n",
        "    scheduler_port=0,\n",
        "    dashboard_address=\":8787\",\n",
        ")\n",
        "client = Client(cluster)\n",
        "client\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Train with Dask XGBoost and Compare Timing\n",
        "Leverage the distributed scheduler with 4 workers and gather runtime plus accuracy for comparison against the pure CPU run with `n_jobs=4`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dX_train = dd.from_pandas(X_train_df, npartitions=4)\n",
        "dy_train = dd.from_pandas(y_train_series, npartitions=4)\n",
        "dX_test = dd.from_pandas(X_test_df, npartitions=4)\n",
        "\n",
        "params = {\n",
        "    \"objective\": objective,\n",
        "    \"tree_method\": \"hist\",\n",
        "    \"eta\": 0.1,\n",
        "    \"max_depth\": 6,\n",
        "    \"subsample\": 0.8,\n",
        "    \"colsample_bytree\": 0.8,\n",
        "    \"eval_metric\": \"logloss\",\n",
        "    \"random_state\": SEED,\n",
        "}\n",
        "if num_classes > 2:\n",
        "    params[\"num_class\"] = num_classes\n",
        "\n",
        "start = time.perf_counter()\n",
        "dtrain = dxgb.DaskDMatrix(client, dX_train, dy_train)\n",
        "dask_output = dxgb.train(client, params, dtrain, num_boost_round=200)\n",
        "dask_elapsed = time.perf_counter() - start\n",
        "part2_results[\"dask_train_time\"] = dask_elapsed\n",
        "print(f\"Dask XGBoost training completed in {dask_elapsed:.2f} seconds.\")\n",
        "\n",
        "booster = dask_output[\"booster\"]\n",
        "booster.set_param({\"predictor\": \"cpu_predictor\"})\n",
        "\n",
        "X_test_np = X_test_df.to_numpy()\n",
        "if objective == \"binary:logistic\":\n",
        "    probas = booster.inplace_predict(X_test_np)\n",
        "    y_pred = (probas > 0.5).astype(int)\n",
        "else:\n",
        "    probas = booster.inplace_predict(X_test_np)\n",
        "    probas = probas.reshape(-1, num_classes)\n",
        "    y_pred = np.argmax(probas, axis=1)\n",
        "\n",
        "dask_acc = accuracy_score(y_test_series, y_pred)\n",
        "part2_results[\"dask_accuracy\"] = dask_acc\n",
        "print(f\"Dask XGBoost accuracy: {dask_acc:.4f}\")\n",
        "\n",
        "comparison_payload = {\n",
        "    \"cpu_n_jobs_4_seconds\": part2_results[\"cpu_train_times\"].get(4),\n",
        "    \"dask_seconds\": dask_elapsed,\n",
        "    \"cpu_n_jobs_4_accuracy\": part2_results[\"cpu_accuracy\"].get(4),\n",
        "    \"dask_accuracy\": dask_acc,\n",
        "}\n",
        "print(json.dumps(comparison_payload, indent=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3 – NYC Flights `dep_delay` Analysis with Dask\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1–3. Load, Clean, and Compute Statistics\n",
        "Install `nycflights13` if needed, drop rows with missing `dep_delay`, and compute mean and standard deviation with a Dask client that uses four workers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f5f9978",
      "metadata": {},
      "outputs": [],
      "source": [
        "from nycflights13 import flights\n",
        "\n",
        "flights_dd = dd.from_pandas(flights, npartitions=16)\n",
        "clean_flights_dd = flights_dd.dropna(subset=[\"dep_delay\"])\n",
        "\n",
        "with performance_report(filename=\"dask_flights_report.html\"):\n",
        "    start = time.perf_counter()\n",
        "    mean_future = clean_flights_dd[\"dep_delay\"].mean()\n",
        "    std_future = clean_flights_dd[\"dep_delay\"].std()\n",
        "    dep_delay_mean, dep_delay_std = dd.compute(mean_future, std_future)\n",
        "    elapsed = time.perf_counter() - start\n",
        "\n",
        "dep_delay_stats = pd.Series({\"mean\": dep_delay_mean, \"std\": dep_delay_std})\n",
        "\n",
        "part3_results[\"dep_delay_stats\"] = dep_delay_stats\n",
        "part3_results[\"dask_runtime_seconds\"] = elapsed\n",
        "\n",
        "dep_delay_stats\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Dashboard / Performance Capture\n",
        "Open the Dask dashboard (link printed in the cell showing the `Client`) and capture a screenshot with “Dask Progress”, “Dask Graph”, and “Dask Task Stream” panels while running the aggregation above. The generated `dask_flights_report.html` can be downloaded as a static record if the live dashboard closes too quickly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Word Report for Parts 2 & 3\n",
        "The cell below creates `homework3_part2_part3.docx` summarizing the key results once all computations have been completed. Re-run if you refresh results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if Document is None:\n",
        "    raise RuntimeError(\"python-docx is unavailable – install it before generating the Word report.\")\n",
        "\n",
        "report_path = OUTPUT_DIR / \"homework3_part2_part3.docx\"\n",
        "doc = Document()\n",
        "doc.add_heading(\"Homework 3 Summary – Parts 2 & 3\", level=1)\n",
        "\n",
        "# Part 2 section\n",
        "doc.add_heading(\"Part 2: XGBoost CPU vs. Dask\", level=2)\n",
        "para = doc.add_paragraph()\n",
        "para.add_run(\"CPU baseline timings (seconds):\\n\").bold = True\n",
        "for n_jobs, seconds in part2_results[\"cpu_train_times\"].items():\n",
        "    para.add_run(f\"  n_jobs={n_jobs}: {seconds:.2f}s, accuracy={part2_results['cpu_accuracy'][n_jobs]:.4f}\\n\")\n",
        "\n",
        "para = doc.add_paragraph()\n",
        "para.add_run(\"Dask XGBoost (4 workers, 1 thread):\\n\").bold = True\n",
        "para.add_run(\n",
        "    f\"  train time: {part2_results['dask_train_time']:.2f}s\\n\"\n",
        ")\n",
        "para.add_run(\n",
        "    f\"  accuracy: {part2_results['dask_accuracy']:.4f}\\n\"\n",
        ")\n",
        "\n",
        "if part2_results[\"cpu_train_times\"].get(4) is not None:\n",
        "    delta = part2_results[\"cpu_train_times\"][4] - part2_results[\"dask_train_time\"]\n",
        "    doc.add_paragraph(f\"Dask XGBoost speed-up vs. CPU n_jobs=4: {delta:.2f} seconds difference.\")\n",
        "\n",
        "# Part 3 section\n",
        "doc.add_heading(\"Part 3: NYC Flights dep_delay Statistics\", level=2)\n",
        "para = doc.add_paragraph()\n",
        "para.add_run(\"Summary statistics:\\n\").bold = True\n",
        "para.add_run(f\"  Mean departure delay: {part3_results['dep_delay_stats']['mean']:.2f} minutes\\n\")\n",
        "para.add_run(f\"  Std. deviation: {part3_results['dep_delay_stats']['std']:.2f} minutes\\n\")\n",
        "para.add_run(f\"  Dask computation time: {part3_results['dask_runtime_seconds']:.2f}s\\n\")\n",
        "\n",
        "para = doc.add_paragraph(\"Dask performance report saved as dask_flights_report.html for documentation.\")\n",
        "\n",
        "doc.save(report_path)\n",
        "print(f\"Saved Word report to {report_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results Summary Snapshot\n",
        "Running the cell below dumps the collected metrics dictionary, which can also be serialized for future reference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "summary = {\n",
        "    \"part1\": part1_results,\n",
        "    \"part2\": part2_results,\n",
        "    \"part3\": {\n",
        "        \"dep_delay_stats\": part3_results.get(\"dep_delay_stats\").to_dict() if part3_results.get(\"dep_delay_stats\") is not None else None,\n",
        "        \"dask_runtime_seconds\": part3_results.get(\"dask_runtime_seconds\"),\n",
        "    },\n",
        "}\n",
        "print(json.dumps(summary, indent=2, default=float))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
